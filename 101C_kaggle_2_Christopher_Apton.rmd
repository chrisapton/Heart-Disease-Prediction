---
title: "Stats 101C - Kaggle Competition 2"
author: "Instructions"
date: "Summer 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(tidymodels)
library(lubridate)
library(glmnet)
library(xgboost)
library(kernlab)
library(LiblineaR)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
```

Data

```{r}
data <- read_csv("heart_train.csv")
```
# worst, removing PE
# alright, removing E
# best, all variables except ID



Cleaning Data


```{r}
data <- read_csv("heart_train.csv")
id_data <- data[, "id"]
data <- select(data, !"id")
data[, "ca"] <- as.numeric(unlist(data[, "ca"]))
data[, "thal"] <- as.numeric(unlist(data[, "thal"]))

# data$cp <- as.character(data$cp)
# data$slope <- as.character(data$slope)
# data$ca <- as.numeric(data$ca)
# dmy <- dummyVars(~., data = data)
# data <- data.frame(predict(dmy, newdata = data))

# data <- data[,-18]
data[, "num"] <- factor(data$num)
data <- data[-which(is.na(data), arr.ind = T)[, 1], ]
head(data)
```

```{r}


```
One hot encoding
```{r}

```

```{r}


```

Splitting Data
```{r}
set.seed(502)
data_split <- initial_split(data, prop = 0.80, strata = num)
train <- training(data_split)
test <- testing(data_split)
```

```{r}
plot(train$num)
```

```{r}
# recipes
norm_recipe <- 
  recipe(num ~ . , data = train) %>%
  # estimate the means and standard deviations
  prep(training = train, retain = TRUE)
norm_recipe
```

```{r}
# models
#lm_model <- linear_reg() %>% set_engine("lm")

boost_model <- boost_tree(mode = "classification", trees = 500, learn_rate = 0.1) %>% 
  set_engine("xgboost")

boost_model2 <- boost_tree(mode = "classification", trees = 200, learn_rate = 0.1) %>% 
  set_engine("xgboost")

svm_model <- svm_poly() %>% 
  set_engine("kernlab") %>% 
  set_mode("classification")

knn_spec <- nearest_neighbor(
    mode = "classification", 
    neighbors = 10
  ) %>%
  set_engine("kknn")

model_list = list(boost_model_1 = boost_model, boost_model_2 = boost_model2, svm_model = svm_model, knn_spec = knn_spec)
```

```{r}
knn_wflow <- 
  workflow() %>% 
  add_model(knn_spec) %>%
  add_recipe(norm_recipe)

knn_res <- 
  tune_grid(
    knn_wflow,
    resamples = ames_folds,
    grid = 4
  )

knn_res
show_best(knn_res)
```


```{r}
#knn_res$.metrics
show_best(knn_res)
```

```{r}
preproc = list(norm = norm_recipe)
glmnet_models <- workflow_set(preproc = preproc, models = model_list)
glmnet_models
```

```{r}
ames_folds <- vfold_cv(train, v = 10)
keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)


glmnet_models <- 
  glmnet_models %>% 
  workflow_map("fit_resamples", 
               seed = 1101, verbose = TRUE,
               resamples = ames_folds, control = keep_pred)
```

```{r}
show_notes(.Last.tune.result)
collect_metrics(glmnet_models)
```

```{r}
autoplot(glmnet_models)
```

```{r}
# importing test data for kaggle
kaggle_data <- read_csv("heart_test.csv")
head(kaggle_data)
```

Cleaning Data

```{r}
id_data <- kaggle_data[, "id"]
kaggle_data <- select(kaggle_data, !"id")
kaggle_data[, "ca"] <- as.numeric(unlist(kaggle_data[, "ca"]))
kaggle_data[, "thal"] <- as.numeric(unlist(kaggle_data[, "thal"]))

kaggle_data[9, "ca"] <- 0
kaggle_data[60, "ca"] <- 0

kaggle_data
```

```{r}
# output for testing
glmnet_wflow <- 
  workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(norm_recipe)

glmnet_fit <- fit(glmnet_wflow, train)

glmnet_wflow_2 <- 
  workflow() %>% 
  add_model(boost_model2) %>% 
  add_recipe(norm_recipe)

glmnet_fit_2 <- fit(glmnet_wflow_2, train)
```


```{r}
# output for kaggle
kaggle_test_results <- bind_cols(id_data, predict(glmnet_fit, new_data = kaggle_data))

kaggle_test_results_2 <- bind_cols(id_data, predict(glmnet_fit_2, new_data = kaggle_data))

names(kaggle_test_results) <- c("Id", "Predicted")

names(kaggle_test_results_2) <- c("Id", "Predicted")

write.csv(kaggle_test_results,"results.csv", row.names = FALSE)

write.csv(kaggle_test_results_2,"results_2.csv", row.names = FALSE)

kaggle_test_results

kaggle_test_results_2
```


```{r}
# Data Exploration for lm model
# glmnet_fit %>% extract_fit_engine() %>% summary() %>% coef()
```
